coach:
  target: coach.GANCoach
  params:
    model_config:
      unet:
        target: src.diffusers.models.unet_2d_timeless.UNetTimeless2DModel
        params:
          sample_size : [512, 512]
          in_channels: 3
          out_channels: 3
          layers_per_block: 2
          block_out_channels: [128, 128, 256, 256, 512, 512]
          down_block_types: [
              DownBlockTimeless2D,
              DownBlockTimeless2D,
              DownBlockTimeless2D,
              DownBlockTimeless2D,
              AttnDownBlockTimeless2D,
              DownBlockTimeless2D]
          up_block_types: [
              UpBlockTimeless2D,
              AttnUpBlockTimeless2D,
              UpBlockTimeless2D,
              UpBlockTimeless2D,
              UpBlockTimeless2D,
              UpDecoderBlockTimeless2D]
      use_ema: False
      ema_max_decay: 0.9999
      ema_inv_gamma: 1.0
      ema_power: 0.75
    loss_config:
      target : losses.i2iloss.I2ILoss
      params:
        lpips_lambda : 0.1
        lpips_type : vgg
        lpips_model_path : /purestorage/project/tyk/project6/predefined_iti_train/pretrained/vgg16-397923af.pth
        lpips_apply: [2000, 999999999]
        clip_lambda : 0.0
        clip_loss_type: clip_sim_loss
        clip_loss_batch : 64
        clip_apply: [2000, 9999999]
        id_lambda : 0.0
        id_backbone_path: /purestorage/project/tyk/project6/predefined_iti_train/pretrained/model_ir_se50.pth 
        id_apply: [0, 0]
        ffl_w: 0.0 # weight for focal frequency loss
        ffl_alpha: 0.0
        ffl_apply: [2000, 99999999] 
    training_config:
      batch_size : 64
      num_epochs: 100 # scheduler 때문에
      optimizer : adam
      learning_rate : 1.0e-04
      adam_beta: 0.95
      adam_beta2: 0.999
      adam_weight_decay: 1e-6
      adam_epsilon: 1e-08
      lr_scheduler: cosine # "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"
      lr_warmup_steps: 500
      snr_gamma: # 5.0