{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from numbers import Number\n",
    "from typing import NamedTuple, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "from config_base import BaseConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a,b,c,d = torch.randn(1, 3, 32, 32).size()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import (conv_nd, linear, normalization, timestep_embedding,\n",
    "                 torch_checkpoint, zero_module)\n",
    "\n",
    "from blocks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BeatGANsUNetConfig(BaseConfig):\n",
    "    image_size: int = 64\n",
    "    in_channels: int = 3\n",
    "    # base channels, will be multiplied\n",
    "    model_channels: int = 64\n",
    "    # output of the unet\n",
    "    # suggest: 3\n",
    "    # you only need 6 if you also model the variance of the noise prediction (usually we use an analytical variance hence 3)\n",
    "    out_channels: int = 3\n",
    "    # how many repeating resblocks per resolution\n",
    "    # the decoding side would have \"one more\" resblock\n",
    "    # default: 2\n",
    "    num_res_blocks: int = 2\n",
    "    # you can also set the number of resblocks specifically for the input blocks\n",
    "    # default: None = above\n",
    "    num_input_res_blocks: int = None\n",
    "    # number of time embed channels and style channels\n",
    "    embed_channels: int = 512\n",
    "    # at what resolutions you want to do self-attention of the feature maps\n",
    "    # attentions generally improve performance\n",
    "    # default: [16]\n",
    "    # beatgans: [32, 16, 8]\n",
    "    attention_resolutions: Tuple[int] = (16, )\n",
    "    # number of time embed channels\n",
    "    time_embed_channels: int = None\n",
    "    # dropout applies to the resblocks (on feature maps)\n",
    "    dropout: float = 0.1\n",
    "    channel_mult: Tuple[int] = (1, 2, 4, 8)\n",
    "    input_channel_mult: Tuple[int] = None\n",
    "    conv_resample: bool = True\n",
    "    # always 2 = 2d conv\n",
    "    dims: int = 2\n",
    "    # don't use this, legacy from BeatGANs\n",
    "    num_classes: int = None\n",
    "    use_checkpoint: bool = False\n",
    "    # number of attention heads\n",
    "    num_heads: int = 1\n",
    "    # or specify the number of channels per attention head\n",
    "    num_head_channels: int = -1\n",
    "    # what's this?\n",
    "    num_heads_upsample: int = -1\n",
    "    # use resblock for upscale/downscale blocks (expensive)\n",
    "    # default: True (BeatGANs)\n",
    "    resblock_updown: bool = True\n",
    "    # never tried\n",
    "    use_new_attention_order: bool = False\n",
    "    resnet_two_cond: bool = False\n",
    "    resnet_cond_channels: int = None\n",
    "    # init the decoding conv layers with zero weights, this speeds up training\n",
    "    # default: True (BeattGANs)\n",
    "    resnet_use_zero_module: bool = True\n",
    "    # gradient checkpoint the attention operation\n",
    "    attn_checkpoint: bool = False\n",
    "\n",
    "    def make_model(self):\n",
    "        return BeatGANsUNetModel(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BeatGANsEncoder(nn.Module):\n",
    "    def __init__(self, conf: BeatGANsUNetConfig):\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "\n",
    "        if conf.num_heads_upsample == -1:\n",
    "            self.num_heads_upsample = conf.num_heads\n",
    "\n",
    "        self.dtype = th.float32\n",
    "\n",
    "        self.time_emb_channels = conf.time_embed_channels or conf.model_channels\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(self.time_emb_channels, conf.embed_channels),\n",
    "            nn.SiLU(),\n",
    "            linear(conf.embed_channels, conf.embed_channels),\n",
    "        )\n",
    "\n",
    "        if conf.num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(conf.num_classes,\n",
    "                                          conf.embed_channels)\n",
    "\n",
    "        ch = input_ch = int(conf.channel_mult[0] * conf.model_channels)\n",
    "        self.input_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(\n",
    "                conv_nd(conf.dims, conf.in_channels, ch, 3, padding=1))\n",
    "        ])\n",
    "\n",
    "        kwargs = dict(\n",
    "            use_condition=True,\n",
    "            two_cond=conf.resnet_two_cond,\n",
    "            use_zero_module=conf.resnet_use_zero_module,\n",
    "            # style channels for the resnet block\n",
    "            cond_emb_channels=conf.resnet_cond_channels,\n",
    "        )\n",
    "\n",
    "        self._feature_size = [ch]\n",
    "\n",
    "        # input_block_chans = [ch]\n",
    "        input_block_chans = [[] for _ in range(len(conf.channel_mult))]\n",
    "        input_block_chans[0].append(ch)\n",
    "\n",
    "        # number of blocks at each resolution\n",
    "        self.input_num_blocks = [0 for _ in range(len(conf.channel_mult))]\n",
    "        self.input_num_blocks[0] = 1\n",
    "        self.output_num_blocks = [0 for _ in range(len(conf.channel_mult))]\n",
    "\n",
    "        ds = 1\n",
    "        resolution = conf.image_size\n",
    "        for level, mult in enumerate(conf.input_channel_mult\n",
    "                                     or conf.channel_mult):\n",
    "            for _ in range(conf.num_input_res_blocks or conf.num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlockConfig(\n",
    "                        ch,\n",
    "                        conf.embed_channels,\n",
    "                        conf.dropout,\n",
    "                        out_channels=int(mult * conf.model_channels),\n",
    "                        dims=conf.dims,\n",
    "                        use_checkpoint=conf.use_checkpoint,\n",
    "                        **kwargs,\n",
    "                    ).make_model()\n",
    "                ]\n",
    "                ch = int(mult * conf.model_channels)\n",
    "                # if resolution in conf.attention_resolutions:\n",
    "                #     layers.append(\n",
    "                #         AttentionBlock(\n",
    "                #             ch,\n",
    "                #             use_checkpoint=conf.use_checkpoint\n",
    "                #             or conf.attn_checkpoint,\n",
    "                #             num_heads=conf.num_heads,\n",
    "                #             num_head_channels=conf.num_head_channels,\n",
    "                #             use_new_attention_order=conf.\n",
    "                #             use_new_attention_order,\n",
    "                #         ))\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size.append(ch)\n",
    "                # input_block_chans.append(ch)\n",
    "                input_block_chans[level].append(ch)\n",
    "                self.input_num_blocks[level] += 1\n",
    "                # print(input_block_chans)\n",
    "            if level != len(conf.channel_mult) - 1:\n",
    "                resolution //= 2\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlockConfig(\n",
    "                            ch,\n",
    "                            conf.embed_channels,\n",
    "                            conf.dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=conf.dims,\n",
    "                            use_checkpoint=conf.use_checkpoint,\n",
    "                            down=True,\n",
    "                            **kwargs,\n",
    "                        ).make_model() if conf.\n",
    "                        resblock_updown else Downsample(ch,\n",
    "                                                        conf.conv_resample,\n",
    "                                                        dims=conf.dims,\n",
    "                                                        out_channels=out_ch)))\n",
    "                ch = out_ch\n",
    "                # input_block_chans.append(ch)\n",
    "                input_block_chans[level + 1].append(ch)\n",
    "                self.input_num_blocks[level + 1] += 1\n",
    "                ds *= 2\n",
    "                self._feature_size.append(ch)\n",
    "\n",
    "        # self._to_vector_layers = [nn.Sequential(\n",
    "        #         normalization(ch),\n",
    "        #         nn.SiLU(),\n",
    "        #         nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        #         conv_nd(conf.dims, ch, ch, 1),\n",
    "        #         nn.Flatten(),\n",
    "        #         ).cuda() for ch in self._feature_size]\n",
    "\n",
    "    def forward(self, x, t=None, y=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param y: an [N] Tensor of labels, if class-conditional.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        # hs = []\n",
    "        hs = [[] for _ in range(len(self.conf.channel_mult))]\n",
    "        #emb = self.time_embed(timestep_embedding(t, self.time_emb_channels))\n",
    "\n",
    "        if self.conf.num_classes is not None:\n",
    "            raise NotImplementedError()\n",
    "            # assert y.shape == (x.shape[0], )\n",
    "            # emb = emb + self.label_emb(y)\n",
    "\n",
    "        # new code supports input_num_blocks != output_num_blocks\n",
    "        h = x.type(self.dtype)\n",
    "        k = 0\n",
    "        results = []\n",
    "        for i in range(len(self.input_num_blocks)):\n",
    "            for j in range(self.input_num_blocks[i]):\n",
    "                h = self.input_blocks[k](h, emb=None)\n",
    "                # print(i, j, h.shape)\n",
    "                hs[i].append(h)\n",
    "                results.append(h)\n",
    "                #print (h.shape)\n",
    "                k += 1\n",
    "        assert k == len(self.input_blocks)\n",
    "\n",
    "        # vectors = []\n",
    "\n",
    "        # for i, feat in enumerate(results):\n",
    "        #     vectors.append(self._to_vector_layers[i](feat))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BeatGANsAutoencConfig(BeatGANsUNetConfig):\n",
    "    # number of style channels\n",
    "    enc_out_channels: int = 512\n",
    "    enc_attn_resolutions: Tuple[int] = None\n",
    "    enc_pool: str = 'depthconv'\n",
    "    enc_num_res_block: int = 2\n",
    "    enc_channel_mult: Tuple[int] = None\n",
    "    enc_grad_checkpoint: bool = False\n",
    "    latent_net_conf= None\n",
    "\n",
    "\n",
    "def get_model_conf():\n",
    "\n",
    "    return BeatGANsAutoencConfig(image_size=256, \n",
    "    in_channels=3,\n",
    "    model_channels=128, \n",
    "    out_channels=3*2,  # also learns sigma\n",
    "    num_res_blocks=2, \n",
    "    num_input_res_blocks=None, \n",
    "    embed_channels=512, \n",
    "    attention_resolutions=(32, 16, 8,), \n",
    "    time_embed_channels=None, \n",
    "    dropout=0.1, \n",
    "    channel_mult=(1, 1, 2, 2, 4, 4), \n",
    "    input_channel_mult=None, \n",
    "    conv_resample=True, \n",
    "    dims=2, \n",
    "    num_classes=None, \n",
    "    use_checkpoint=False,\n",
    "    num_heads=1, \n",
    "    num_head_channels=-1, \n",
    "    num_heads_upsample=-1, \n",
    "    resblock_updown=True, \n",
    "    use_new_attention_order=False, \n",
    "    resnet_two_cond=True, \n",
    "    resnet_cond_channels=None, \n",
    "    resnet_use_zero_module=True, \n",
    "    attn_checkpoint=False, \n",
    "    enc_out_channels=512, \n",
    "    enc_attn_resolutions=None, \n",
    "    enc_pool='adaptivenonzero', \n",
    "    enc_num_res_block=2, \n",
    "    enc_channel_mult=(1, 1, 2, 2, 4, 4, 4), \n",
    "    enc_grad_checkpoint=False, )\n",
    "    # latent_net_conf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeatGANsAutoencConfig().latent_net_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_model_conf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "be = BeatGANsEncoder(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = be(torch.randn(1, 3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 128, 512, 512])\n",
      "1 torch.Size([1, 128, 512, 512])\n",
      "2 torch.Size([1, 128, 512, 512])\n",
      "3 torch.Size([1, 128, 256, 256])\n",
      "4 torch.Size([1, 128, 256, 256])\n",
      "5 torch.Size([1, 128, 256, 256])\n",
      "6 torch.Size([1, 128, 128, 128])\n",
      "7 torch.Size([1, 256, 128, 128])\n",
      "8 torch.Size([1, 256, 128, 128])\n",
      "9 torch.Size([1, 256, 64, 64])\n",
      "10 torch.Size([1, 256, 64, 64])\n",
      "11 torch.Size([1, 256, 64, 64])\n",
      "12 torch.Size([1, 256, 32, 32])\n",
      "13 torch.Size([1, 512, 32, 32])\n",
      "14 torch.Size([1, 512, 32, 32])\n",
      "15 torch.Size([1, 512, 16, 16])\n",
      "16 torch.Size([1, 512, 16, 16])\n",
      "17 torch.Size([1, 512, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(a):\n",
    "    print(i, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiteam/miniconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.diffusers.models.unet_2d_base import UNet2DBaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = UNet2DBaseModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 u(torch.randn(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">256</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">256</span>), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aiteam/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/aiteam/tykim/cubox/diffusers/src/diffusers/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">unet_2d_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">256</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">253 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>hidden_states=sample, temb=emb, skip_sample=skip_sample                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">254 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">255 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>256 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>sample, res_samples = downsample_block(hidden_states=sample, temb=emb)     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">257 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">258 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>down_block_res_samples += res_samples                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">259 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'emb'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 u(torch.randn(\u001b[94m1\u001b[0m, \u001b[94m3\u001b[0m, \u001b[94m256\u001b[0m, \u001b[94m256\u001b[0m), \u001b[94m3\u001b[0m)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/aiteam/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/aiteam/tykim/cubox/diffusers/src/diffusers/models/\u001b[0m\u001b[1;33munet_2d_base.py\u001b[0m:\u001b[94m256\u001b[0m in \u001b[92mforward\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states=sample, temb=emb, skip_sample=skip_sample                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m255 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m256 \u001b[2m│   │   │   │   \u001b[0msample, res_samples = downsample_block(hidden_states=sample, temb=emb)     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m257 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m258 \u001b[0m\u001b[2m│   │   │   \u001b[0mdown_block_res_samples += res_samples                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m259 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'emb'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u(torch.randn(1, 3, 256, 256), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
